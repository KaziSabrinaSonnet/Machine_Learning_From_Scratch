{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# always import\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# numpy & scipy\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import pairwise_distances_argmin, pairwise_distances\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Hungarian algorithm\n",
    "from munkres import Munkres\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "\n",
    "# maybe\n",
    "from numba import jit\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# load MNIST data and normalization\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, data_home='mnist/')\n",
    "y = np.asarray(list(map(int, y)))\n",
    "X = np.asarray(X.astype(float))\n",
    "X = scale(X)\n",
    "n_digits = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Kmeans\n",
    "#2(a)-i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "class kmeans():\n",
    "\n",
    "    def __init__(self, K= n_digits, maximum_iteration = 300, tau = 0.0001):\n",
    "        self.K = K\n",
    "        self.maximum_iteration = maximum_iteration\n",
    "        self.tau = tau\n",
    "        self.clusters = [[] for i in range (self.K)]\n",
    "        self.centroids = []\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        self.samples = data.shape[0]\n",
    "        self.feature = data.shape[1]\n",
    "        self.centroids= self.initialize_centroid(data)\n",
    "        previous_objective = 0\n",
    "        iteration = 0\n",
    "        for i in range (self.maximum_iteration):\n",
    "            self.clusters = self.cluster_create(self.centroids)\n",
    "            previous_centroid = self.centroids\n",
    "            self.centroids = self.calculate_new_centroids(self.clusters)\n",
    "            current_objective = self.calculate_objective_function(self.centroids)\n",
    "            if abs(previous_objective - current_objective ) < self.tau:\n",
    "                break\n",
    "            previous_objective =current_objective\n",
    "            iteration = iteration+1\n",
    "        print(iteration)\n",
    "        return self.cluster_label(self.clusters)\n",
    "\n",
    "    def initialize_centroid(self, data):\n",
    "        pca = PCA()\n",
    "        pca = pca.fit(data)\n",
    "        initial_centroid = pca.components_[:10]\n",
    "        initial_centroid_list = list()\n",
    "        for i in range(initial_centroid.shape[0]):\n",
    "            initial_centroid_list.append(initial_centroid[i, :])\n",
    "        return initial_centroid_list\n",
    "\n",
    "    def cluster_create(self, centroids ):\n",
    "        clusters = [[] for i in range(self.K)]\n",
    "        for index, sample in enumerate(self.data):\n",
    "            sample2centroid_distance = [euclidean_distance(sample, c) for c in centroids]\n",
    "            clusters[np.argmin(sample2centroid_distance)].append(index)\n",
    "        return clusters\n",
    "    \n",
    "    def calculate_objective_function(self, centroids):\n",
    "        sum_square_distance = list()\n",
    "        for index, sample in enumerate(self.data):\n",
    "            sample2centroid_distance = [euclidean_distance(sample, c) for c in centroids]\n",
    "            sum_square_distance.append(np.min(sample2centroid_distance))\n",
    "        return np.sum(np.square(sum_square_distance))\n",
    "        \n",
    "    def calculate_new_centroids(self, clusters):\n",
    "        centroids = np.zeros((self.K, self.feature))\n",
    "        for index, cluster in enumerate(clusters):\n",
    "            meanOfCluster = np.mean(self.data[cluster], axis=0)\n",
    "            centroids[index] = meanOfCluster\n",
    "        return centroids\n",
    "    \n",
    "    def cluster_label(self, clusters):\n",
    "        labels = np.empty (self.samples)\n",
    "        for index, cluster in enumerate(clusters):\n",
    "            for item in cluster:\n",
    "                labels[item] = index\n",
    "        return labels\n",
    "    \n",
    "    def fit1(self, data):\n",
    "        j= -1\n",
    "        objective = [[] for i in range(10)]\n",
    "        prediction = [[] for i in range(10)]\n",
    "        for l in range (10):\n",
    "            j= j+1\n",
    "            self.data = data\n",
    "            self.samples = data.shape[0]\n",
    "            self.feature = data.shape[1]\n",
    "            seedValue = random.randrange(sys.maxsize)\n",
    "            random.seed(seedValue)\n",
    "            random_sample = np.random.choice(self.samples, self.K, replace=False)\n",
    "            self.centroids = [self.data[index] for index in random_sample]\n",
    "            previous_objective = 0\n",
    "            for i in range (self.maximum_iteration):\n",
    "                self.clusters = self.cluster_create(self.centroids)\n",
    "                previous_centroid = self.centroids\n",
    "                self.centroids = self.calculate_new_centroids(self.clusters)\n",
    "                current_objective = self.calculate_objective_function(self.centroids)\n",
    "                if abs(previous_objective - current_objective ) < self.tau:\n",
    "                    break\n",
    "                previous_objective =current_objective\n",
    "            objective[j].append(current_objective)\n",
    "            prediction[j].append(self.cluster_label(self.clusters))   \n",
    "        return objective, prediction\n",
    "\n",
    "    def best_centroid_4KNN(self, data):\n",
    "        self.data = data\n",
    "        self.samples = data.shape[0]\n",
    "        self.feature = data.shape[1]\n",
    "        random_sample = np.random.choice(self.samples, self.K, replace=False)\n",
    "        self.centroids = [self.data[index] for index in random_sample]\n",
    "        previous_objective = 0\n",
    "        for i in range (self.maximum_iteration):\n",
    "            self.clusters = self.cluster_create(self.centroids)\n",
    "            previous_centroid = self.centroids\n",
    "            self.centroids = self.calculate_new_centroids(self.clusters)\n",
    "            current_objective = self.calculate_objective_function(self.centroids)\n",
    "            if abs(previous_objective - current_objective ) < self.tau:\n",
    "                break\n",
    "            previous_objective =current_objective\n",
    "        return self.centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = kmeans(K= n_digits, maximum_iteration = 300, tau = 0.0001)\n",
    "y_pred = k.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "print(\"%.6f\" % homogeneity_score(y, y_pred))\n",
    "print(\"%.6f\" % completeness_score(y, y_pred))\n",
    "print(\"%.6f\" % v_measure_score(y, y_pred))\n",
    "print(\"%.6f\" % adjusted_mutual_info_score(y, y_pred))\n",
    "print(\"%.6f\" % adjusted_rand_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#2a(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=30)\n",
    "X_dash = pca.fit_transform(X)\n",
    "k = kmeans(K= n_digits, maximum_iteration = 300, tau = 0.0001)\n",
    "ob, pred = k.fit1(X_dash)\n",
    "smallest_objective = np.argmin(ob)\n",
    "y_pred1 = pred[smallest_objective]\n",
    "y_pred1 = np.array(y_pred1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "print(\"%.6f\" % homogeneity_score(y, y_pred1))\n",
    "print(\"%.6f\" % completeness_score(y, y_pred1))\n",
    "print(\"%.6f\" % v_measure_score(y, y_pred1))\n",
    "print(\"%.6f\" % adjusted_mutual_info_score(y, y_pred1))\n",
    "print(\"%.6f\" % adjusted_rand_score(y, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Hungarian starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def cost_matrix(y_pred, y, k):\n",
    "    size_y = len(y)\n",
    "    init_cost_mat = np.zeros((k, k))\n",
    "    for i in range (k):\n",
    "        for j in range (k):\n",
    "            bool_array = np.logical_and(y_pred == i, y == j)\n",
    "            init_cost_mat[i][j] = np.sum(bool_array)\n",
    "    return (1-init_cost_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# y_pred1\n",
    "mun= Munkres()\n",
    "index = mun.compute(cost_matrix(y_pred1, y, n_digits))\n",
    "mp = {prev: cur for (prev, cur) in index}\n",
    "munkres_label = np.array([mp[i] for i in y_pred1 ])\n",
    "cnf_mat = confusion_matrix(y, munkres_label, labels=range(n_digits))\n",
    "accuracy = np.trace(cnf_mat, dtype=float) / np.sum(cnf_mat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# y_pred\n",
    "mun= Munkres()\n",
    "index = mun.compute(cost_matrix(y_pred, y, n_digits))\n",
    "mp = {prev: cur for (prev, cur) in index}\n",
    "munkres_label = np.array([mp[i] for i in y_pred ])\n",
    "cnf_mat = confusion_matrix(y, munkres_label, labels=range(n_digits))\n",
    "accuracy = np.trace(cnf_mat, dtype=float) / np.sum(cnf_mat) \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Spectral_Clustering_Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "def data_initialization(X):\n",
    "    pca = PCA(n_components=30)\n",
    "    X_dash = pca.fit_transform(X)\n",
    "    return X_dash\n",
    "\n",
    "X_dash = data_initialization(X)\n",
    "\n",
    "def sparse_distance(X_dash):\n",
    "    E = NearestNeighbors(n_neighbors=500, algorithm= 'kd_tree', metric= 'euclidean')\n",
    "    E= E.fit(X_dash).kneighbors_graph(mode = 'distance')\n",
    "    return E\n",
    "\n",
    "H = sparse_distance(X_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from scipy import sparse\n",
    "def sigma(H):\n",
    "    sum_H = H.sum()\n",
    "    Nh= H.nonzero()\n",
    "    H_mod = len(Nh[0])\n",
    "    sig = sum_H/H_mod\n",
    "    return sig\n",
    "\n",
    "sig = sigma(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def similarity_matrix(H, sig):\n",
    "    np.square(H.data, out = H.data)\n",
    "    sig2 = sig**2\n",
    "    np.divide(H.data, sig2, out = H.data)\n",
    "    np.exp(-(H.data), out= H.data)\n",
    "    S = sparse.csr_matrix(1/H.sum(1))\n",
    "    E = H.multiply(S)\n",
    "    E.setdiag(1.0)\n",
    "    ET = E.transpose()\n",
    "    E= (E+ET)/2\n",
    "    return E\n",
    "\n",
    "E = similarity_matrix(H, sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def degree_matrix(E):\n",
    "    diag = np.squeeze(np.asarray(E.sum(1)))\n",
    "    D= sparse.diags(diag, format='csr')\n",
    "    return D\n",
    "\n",
    "D = degree_matrix(E)\n",
    "\n",
    "def calculate_laplacian(E, D):\n",
    "    I = sparse.identity(E.shape[0])\n",
    "    np.power(D.data, -0.5, out= D.data)\n",
    "    DED = (D.dot(E)).dot(D)\n",
    "    L= I-DED\n",
    "    return L\n",
    "\n",
    "L = calculate_laplacian(E, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigs\n",
    "vals1, vecs1 = sparse.linalg.eigs(L, k= 20, which= 'SM')\n",
    "\n",
    "def normalizing_eigenvector(vecs):\n",
    "    vecs = vecs/np.linalg.norm(vecs, axis= 1).reshape(-1, 1)\n",
    "    return vecs\n",
    "\n",
    "vecs1 = normalizing_eigenvector(vecs1)\n",
    "sorted_vecs = np.delete(vecs1, 0, 1).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans= KMeans(n_clusters=10, init='k-means++', n_init=10).fit(sorted_vecs)\n",
    "mun= Munkres()\n",
    "index = mun.compute(cost_matrix(kmeans.labels_, y, 10))\n",
    "mp = {prev: cur for (prev, cur) in index}\n",
    "munkres_label = np.array([mp[i] for i in kmeans.labels_])\n",
    "cnf_mat = confusion_matrix(y, munkres_label, labels=range(10))\n",
    "accuracy = np.trace(cnf_mat, dtype=float) / np.sum(cnf_mat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#K_Nearest_Neighbour_Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Knearest:\n",
    "    def __init__(self, X, y, k):\n",
    "        self.k = k\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    def predict(self,X):\n",
    "        y_pred = np.array([self.fit(sample) for sample in X])\n",
    "        return y_pred\n",
    "    def fit(self, X):\n",
    "        nearest_neighbour_index = np.argsort([euclidean_distance(X, x_train) for x_train in self.X_train])[:self.k]\n",
    "        nearest_neighbour_label = [self.y_train[sample] for sample in nearest_neighbour_index]  \n",
    "        voting = Counter(nearest_neighbour_label).most_common(1)\n",
    "        return voting [0][0]\n",
    "def accuracy(true, pred):\n",
    "    accuracy = np.sum(true == pred) / len(true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Kmeans method\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=30)\n",
    "X_dash = pca.fit_transform(X)\n",
    "k = kmeans(K= 100, maximum_iteration = 300, tau = 0.0001)\n",
    "centroids = k.best_centroid_4KNN(X_dash)\n",
    "centroids = np.array(centroids)\n",
    "X_dash = X_dash.tolist()\n",
    "selected_X_dash = []\n",
    "l1= list(range(70000))\n",
    "l2 = []\n",
    "for index, sample in enumerate(centroids):\n",
    "    sample2centroid_distance = [euclidean_distance(c, sample) for c in X_dash]\n",
    "    selected_X_dash.append(X_dash[np.argmin(sample2centroid_distance)])\n",
    "    l2.append(np.argmin(sample2centroid_distance))\n",
    "l3 = [x for x in l1 if x not in l2]\n",
    "X_test = np.array([X_dash[l] for l in l3])\n",
    "X_train = np.array(selected_X_dash)\n",
    "y_test = np.array([y[l] for l in l3]) \n",
    "y_train = np.array([y[l] for l in l2]) \n",
    "from collections import Counter\n",
    "Accuracy = []\n",
    "for i in range(1, 6, 2):\n",
    "    classification = Knearest(X = X_train, y= y_train, k= i)\n",
    "    pred = classification.predict(X_test)\n",
    "    acc = accuracy(y_test, pred)\n",
    "    Accuracy.append(acc)\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Spectral clustering method\n",
    "k1 = kmeans(K= 100, maximum_iteration = 300, tau = 0.0001)\n",
    "centroids1 = k1.best_centroid_4KNN(sorted_vecs)\n",
    "centroids1 = np.array(centroids1)\n",
    "sorted_vecs = sorted_vecs.tolist()\n",
    "selected_vecs1 = []\n",
    "k1= list(range(70000))\n",
    "k2 = []\n",
    "for index, sample in enumerate(centroids1):\n",
    "    sample2centroid_distance = [euclidean_distance(c, sample) for c in sorted_vecs]\n",
    "    selected_vecs1.append(sorted_vecs[np.argmin(sample2centroid_distance)])\n",
    "    k2.append(np.argmin(sample2centroid_distance))\n",
    "k3 = [x for x in k1 if x not in k2]\n",
    "X_test1 = np.array([sorted_vecs[k] for k in k3])\n",
    "X_train1 = np.array(selected_vecs1)\n",
    "y_test1 = np.array([y[k] for k in k3]) \n",
    "y_train1 = np.array([y[k] for k in k2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from collections import Counter\n",
    "Accuracy1 = []\n",
    "for i in range(1, 6, 2):\n",
    "    classification = Knearest(X = X_train1, y= y_train1, k= i)\n",
    "    pred1 = classification.predict(X_test1)\n",
    "    acc1 = accuracy(y_test1, pred1)\n",
    "    Accuracy1.append(acc1)\n",
    "print(Accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Random Sampling\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=30)\n",
    "X_dash = pca.fit_transform(X)\n",
    "sample2 = X_dash.shape[0]\n",
    "random_sample2 = np.random.choice(sample2, 100, replace=False).tolist()\n",
    "X_train2 = np.array([X_dash[index] for index in random_sample2])\n",
    "m1= list(range(70000))\n",
    "m2 = [x for x in m1 if x not in random_sample2]\n",
    "X_test2 = np.array([X_dash[m] for m in m2])\n",
    "y_test2 = np.array([y[k] for k in m2]) \n",
    "y_train2 = np.array([y[index] for index in random_sample2])\n",
    "from collections import Counter\n",
    "Accuracy2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "for i in range(1, 6, 2):\n",
    "    classification = Knearest(X = X_train2, y= y_train2, k= i)\n",
    "    pred2 = classification.predict(X_test2)\n",
    "    acc2 = accuracy(y_test2, pred2)\n",
    "    Accuracy2.append(acc2)\n",
    "print(Accuracy2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
