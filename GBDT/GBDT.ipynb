{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        gradient = -2*(true-score)\n",
    "        return gradient\n",
    "\n",
    "    def h(self,true,score):\n",
    "        hessian = np.repeat(2, len(true))\n",
    "        return hessian\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,score):\n",
    "        Pr1 = 1/(1+np.exp(-score))\n",
    "        prdc = [1 if item >= 0.5 else 0 for item in Pr1]\n",
    "        return np.array(prdc)\n",
    "\n",
    "    def g(self,true,score):\n",
    "        var1 = np.exp(score)\n",
    "        var2 = np.exp(-score)\n",
    "        gradient = (-true/(1+var1))+((1-true)/(1+var2))\n",
    "        return gradient\n",
    "    def h(self,true,score):\n",
    "        var1 = np.exp(score)\n",
    "        var2 = np.exp(-score)\n",
    "        hessian = ((true*var1)/(1+var1)**2)+ (((1-true)*var2)/(1+var2)**2)\n",
    "        return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(train, target):\n",
    "        samples = train.shape[0]\n",
    "        index = np.random.choice(samples, samples, replace=True)\n",
    "        return train[index], target[index]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "        \n",
    "    '''\n",
    "\n",
    "    def __init__(self, loss = 'mse',\n",
    "        max_depth = 10, min_sample_split = 2, \n",
    "        lamda = 0.2, gamma = 0.1,\n",
    "        rf = 0.5, num_trees = 20, feat = None):\n",
    "        \n",
    "        #self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.feat = feat\n",
    "        self.tree= []\n",
    "    \n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        self.tree  = []\n",
    "        print(f'Fitting {self.num_trees} number of trees for RF')\n",
    "        for x in range(self.num_trees):\n",
    "            print(f'Tree number: {x}')\n",
    "            single_tree = Tree(rf= self.rf, loss= self.loss, max_depth=  self.max_depth, min_sample_split=self.min_sample_split,\n",
    "                lamda=self.lamda, gamma = self.gamma, feat = self.feat)\n",
    "            sample_x, sample_y = bootstrap(train, target)\n",
    "            single_tree.fit(sample_x, sample_y)\n",
    "            self.tree.append(single_tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "        predicted_tree = np.array([tree.predict(test) for tree in self.tree])\n",
    "        predicted_tree = np.swapaxes(predicted_tree, 0, 1)\n",
    "        if self.loss == 'mse':\n",
    "            y_pred = [np.mean(pt) for pt in predicted_tree]\n",
    "        if self.loss == 'logistic':\n",
    "            y_hat_score = np.mean(predicted_tree, axis=1)\n",
    "            y_pred = logistic.pred(self, y_hat_score)\n",
    "        \n",
    "        return np.array(y_pred)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth D_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        learning_rate: The learning rate eta of GBDT.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 2, \n",
    "        lamda = 0.2, gamma = 0.1,\n",
    "        learning_rate = 0.1, num_trees = 100, rf=1):\n",
    "        \n",
    "        #self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        self.rf = rf\n",
    "        self.boosting_tree = []\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "\n",
    "        self.boosting_tree  = []\n",
    "        print(f'Fitting {self.num_trees} number of trees for GBDT')\n",
    "        for x in range(self.num_trees):\n",
    "            print(f'Tree number: {x}')\n",
    "            single_tree = Tree(rf= self.rf, loss= self.loss, max_depth=  self.max_depth, min_sample_split=self.min_sample_split,\n",
    "                lamda=self.lamda, gamma = self.gamma, feat = None, lr= self.learning_rate)\n",
    "            single_tree.fit(train, target, list_prev_tree=self.boosting_tree)\n",
    "            self.boosting_tree.append(single_tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        predicted_boosting_tree = np.array([tree.predict(test) for tree in self.boosting_tree])\n",
    "        predicted_tree = np.swapaxes(predicted_boosting_tree, 0, 1)\n",
    "        if self.loss == 'mse':\n",
    "            y_pred = [np.sum(pt) for pt in predicted_tree]\n",
    "        if self.loss == 'logistic':\n",
    "            y_hat_score = np.sum(predicted_tree, axis=1)\n",
    "            y_pred = logistic.pred(self, y_hat_score)\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    \n",
    "    #TODO\n",
    "    def __init__(self, feature=None, threshold=None, left_child=None, right_child=None, *, leaf_value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.leaf_value = leaf_value\n",
    "        #self.is_leaf = False\n",
    "        \n",
    "        #[X1, X2, index_y, value_y] = split(X)\n",
    "        #self.left_child = TreeNode(X1)\n",
    "        #self.right_child = TreeNode(X2)\n",
    "    \"\"\"    \n",
    "    def forward(self, x):\n",
    "        if x[index_y] < value_y:\n",
    "            return self.left_child\n",
    "    \"\"\"    \n",
    "    def is_leaf_node(self):\n",
    "        return self.leaf_value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, rf, loss, max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 2, gamma = 0.3, feat = None, lr= 0.2):\n",
    "        #self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.int_member = 0\n",
    "        self.feat = feat\n",
    "        self.root = None\n",
    "        self.loss = loss \n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, train_x, train_y, list_prev_tree=None):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        #TODO\n",
    "        self.feat = int(train_x.shape[1] * self.rf)\n",
    "        self.root = self.construct_tree(train_x, train_y, list_prev_tree=list_prev_tree)\n",
    "        \n",
    "\n",
    "    def predict(self,test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        #TODO\n",
    "        result = np.array([self.travel_tree(t, self.root) for t in test])\n",
    "        return result\n",
    "\n",
    "    def construct_tree(self, train_x, train_y, depth= 0, gain = 1, list_prev_tree=None):\n",
    "        '''\n",
    "        Tree construction, which is recursively used to grow a tree.\n",
    "        First we should check if we should stop further splitting.\n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "        '''\n",
    "        #TODO\n",
    "        samples, features = train_x.shape\n",
    "        labels = len(np.unique(train_y))\n",
    "        if (depth >= self.max_depth or labels == 1 or samples < self.min_sample_split or gain<= 0):\n",
    "            leaf = self.find_wk(train_x, train_y, self.loss, self.lamda, list_prev_tree)\n",
    "            return TreeNode(leaf_value=leaf)\n",
    "        feature_index = list(range(features))\n",
    "        if self.rf != 1:\n",
    "            feature_index = np.random.choice(features, self.feat, replace= False)\n",
    "        best_feature, best_threshold, gain = self.find_best_decision_rule(train_x, train_y, feature_index, self.loss, self.lamda, self.gamma, self.lr, self.rf, list_prev_tree)\n",
    "        left_child_index, right_child_index = self.split(train_x[:, best_feature], best_threshold) \n",
    "        left_child = self.construct_tree(train_x[left_child_index, :], train_y[left_child_index], depth+1, gain, list_prev_tree)\n",
    "        right_child = self.construct_tree(train_x[right_child_index, :], train_y[right_child_index], depth+1, gain, list_prev_tree)\n",
    "\n",
    "        return TreeNode(best_feature, best_threshold, left_child, right_child)\n",
    "    \n",
    "\n",
    "    def find_best_decision_rule(self, train_x, train_y, feature_index, loss, lamda, gamma, lr, rf, list_prev_tree=None):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        #TODO\n",
    "        if rf !=1:\n",
    "            X_selected = train_x[:, feature_index]\n",
    "        else:\n",
    "            X_selected = train_x\n",
    "        threshold = list()\n",
    "        for feature in range(X_selected.shape[1]):\n",
    "            sort = (np.sort(X_selected[:, feature])).tolist()\n",
    "            i= 0\n",
    "            temp_threshold = []\n",
    "            for item in range (len(sort)-1):\n",
    "                temp_threshold.append((sort[i]+sort[i+1])/2)    \n",
    "                i= i+1\n",
    "            threshold.append(temp_threshold)\n",
    "        Fin_G, Fin_T = self.find_threshold(X_selected, train_y, threshold, loss, lamda, gamma, lr, list_prev_tree)\n",
    "        max_gain_index = Fin_G.index(max(Fin_G))\n",
    "        best_feature = max_gain_index\n",
    "        best_threshold = Fin_T[max_gain_index]\n",
    "        return best_feature, best_threshold, max(Fin_G)\n",
    "    \n",
    "    def get_current_pred(self, feature_set, labels, list_prev_tree, bool_rf):\n",
    "        tree_pred = np.repeat(0, len(labels))\n",
    "        if bool_rf:\n",
    "            return tree_pred\n",
    "        for tree in list_prev_tree:\n",
    "            tree_pred = np.add(tree_pred, tree.predict(feature_set))\n",
    "        return tree_pred\n",
    "\n",
    "    def find_threshold(self, X_selected, train_y, threshold, loss, lamda, gamma, lr, list_prev_tree=None):\n",
    "        '''\n",
    "        Given a particular feature $p_j$,\n",
    "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        Fin_G = list()\n",
    "        Fin_T = list()\n",
    "        #TODO \n",
    "        i= -1\n",
    "        for feature in range (X_selected.shape[1]):\n",
    "            xj = X_selected[:, feature].tolist()\n",
    "            G = list()\n",
    "            T =list()\n",
    "            i = i+1\n",
    "            for item in threshold[i]:\n",
    "                list_left_index = [idx for idx in range(len(train_y)) if xj[idx] < item]\n",
    "                list_right_index = [idx for idx in range(len(train_y)) if xj[idx] >= item]\n",
    "                x_left = X_selected[list_left_index]\n",
    "                y_left = train_y[list_left_index]\n",
    "                x_right = X_selected[list_right_index]\n",
    "                y_right = train_y[list_right_index]\n",
    "                yhat_left = self.get_current_pred(x_left, y_left, list_prev_tree, self.rf != 1)\n",
    "                yhat_right = self.get_current_pred(x_right, y_right, list_prev_tree, self.rf != 1)\n",
    "                if self.loss == 'mse':\n",
    "                    gjL = leastsquare.g(self, np.array(y_left), yhat_left)\n",
    "                    gjL = np.sum(gjL)\n",
    "                    hjL = leastsquare.h(self, np.array(y_left), yhat_left)\n",
    "                    hjL = np.sum(hjL)\n",
    "                    gjR = leastsquare.g(self, np.array(y_right), yhat_right)\n",
    "                    gjR = np.sum(gjR)\n",
    "                    hjR = leastsquare.h(self, np.array(y_right), yhat_right)\n",
    "                    hjR = np.sum(hjR)\n",
    "                if self.loss == 'logistic':\n",
    "                    gjL = logistic.g(self, np.array(y_left), yhat_left)\n",
    "                    gjL = np.sum(gjL)\n",
    "                    hjL = logistic.h(self, np.array(y_left), yhat_left)\n",
    "                    hjL = np.sum(hjL)\n",
    "                    gjR = logistic.g(self, np.array(y_right), yhat_right)\n",
    "                    gjR = np.sum(gjR)\n",
    "                    hjR = logistic.h(self, np.array(y_right), yhat_right)\n",
    "                    hjR = np.sum(hjR)\n",
    "                Gain = 0.5* (((gjL**2)/(hjL+lamda)) + ((gjR**2)/(hjR+lamda))-(((gjL+gjR)**2)/(hjL+hjR+lamda)))-gamma\n",
    "                G.append(Gain)\n",
    "                T.append(item)\n",
    "            max_gain = max(G)\n",
    "            max_gain_index = G.index(max_gain)\n",
    "            selected_threshold = T[max_gain_index]\n",
    "            Fin_G.append(max_gain)\n",
    "            Fin_T.append(selected_threshold)\n",
    "        return Fin_G, Fin_T\n",
    "\n",
    "\n",
    "    def find_wk(self, train_x, train_y, loss, lamda, list_prev_tree=None):\n",
    "        yhat = self.get_current_pred(train_x, train_y, list_prev_tree, self.rf != 1)\n",
    "        if self.loss == 'mse':\n",
    "            gj = leastsquare.g(self, train_y, yhat)\n",
    "            hj = leastsquare.h(self, train_y, yhat)\n",
    "        if self.loss == 'logistic':\n",
    "            gj = logistic.g(self, train_y, yhat)\n",
    "            hj = logistic.h(self, train_y, yhat)\n",
    "        Gj = np.sum(gj)\n",
    "        Hj = np.sum(hj)\n",
    "        Wk = -Gj/(Hj+lamda)\n",
    "        return Wk\n",
    "\n",
    "    def travel_tree(self, train_x, treenode):\n",
    "        if treenode.is_leaf_node():\n",
    "            return treenode.leaf_value\n",
    "        elif train_x[treenode.feature] <= treenode.threshold:\n",
    "            return self.travel_tree(train_x, treenode.left_child)\n",
    "        else:\n",
    "            return self.travel_tree(train_x, treenode.right_child)\n",
    "\n",
    "    \n",
    "\n",
    "    def split(self, train_x, best_threshold):\n",
    "        left_child_index = np.argwhere(train_x <= best_threshold).flatten()\n",
    "        right_child_index = np.argwhere(train_x > best_threshold).flatten()\n",
    "        return left_child_index, right_child_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "def root_mean_square_error(pred, y):\n",
    "   rmse = np.sqrt(np.square(np.subtract(y, pred)).mean()) \n",
    "   return rmse\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    return np.sum(y==pred)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "list_train_metric = []\n",
    "list_test_metric = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 35 number of trees for RF\n",
      "Tree number: 0\n",
      "Tree number: 1\n",
      "Tree number: 2\n",
      "Tree number: 3\n",
      "Tree number: 4\n",
      "Tree number: 5\n",
      "Tree number: 6\n",
      "Tree number: 7\n",
      "Tree number: 8\n",
      "Tree number: 9\n",
      "Tree number: 10\n",
      "Tree number: 11\n",
      "Tree number: 12\n",
      "Tree number: 13\n",
      "Tree number: 14\n",
      "Tree number: 15\n",
      "Tree number: 16\n",
      "Tree number: 17\n",
      "Tree number: 18\n",
      "Tree number: 19\n",
      "Tree number: 20\n",
      "Tree number: 21\n",
      "Tree number: 22\n",
      "Tree number: 23\n",
      "Tree number: 24\n",
      "Tree number: 25\n",
      "Tree number: 26\n",
      "Tree number: 27\n",
      "Tree number: 28\n",
      "Tree number: 29\n",
      "Tree number: 30\n",
      "Tree number: 31\n",
      "Tree number: 32\n",
      "Tree number: 33\n",
      "Tree number: 34\n",
      "Train RMSE: 5.629730888762528\n",
      "Test RMSE: 5.584646976575847\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# TODO: RF regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "clf = RF(loss='mse', num_trees=35)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "RMSE_train = root_mean_square_error(y_pred_train, y_train)\n",
    "print (\"Train RMSE:\", RMSE_train)\n",
    "list_train_metric.append(RMSE_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "RMSE_test = root_mean_square_error(y_pred_test, y_test)\n",
    "print (\"Test RMSE:\", RMSE_test)\n",
    "list_test_metric.append(RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: RF classification on credit-g dataset\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "clf = RF(loss='logistic', num_trees=35)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "acc_train = accuracy(y_train, y_pred_train)\n",
    "print (\"Train Accuracy:\", acc_train)\n",
    "list_train_metric.append(acc_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "acc_test = accuracy(y_test, y_pred_test)\n",
    "print (\"Test Accuracy:\", acc_test)\n",
    "list_test_metric.append(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: RF classification on breast cancer dataset\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "clf = RF(loss='logistic', num_trees=35)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "acc_train = accuracy(y_train, y_pred_train)\n",
    "print (\"Train Accuracy:\", acc_train)\n",
    "list_train_metric.append(acc_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "acc_test = accuracy(y_test, y_pred_test)\n",
    "print (\"Test Accuracy:\", acc_test)\n",
    "list_test_metric.append(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: Least Square \n",
    "# load data\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "import numpy as np\n",
    "def least_square(X, y):\n",
    "    #theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "    theta = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)\n",
    "    return theta\n",
    "def ridge_reg(X, y, eta):\n",
    "    n, m = X.shape\n",
    "    I= np.identity(m)\n",
    "    theta_r = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X) + (eta/2) * I), X.T), y)\n",
    "    return theta_r\n",
    "def pred_fn(X, theta):\n",
    "    pred = np.dot(X, theta)\n",
    "    return pred\n",
    "def root_mean_square_error(pred, y):\n",
    "   rmse = np.sqrt(np.square(np.subtract(y, pred)).mean()) \n",
    "   return rmse\n",
    "#Linear Regression\n",
    "train_offset = np.ones((len(X_train), 1), dtype = np.float64)\n",
    "test_offset = np.ones((len(X_test), 1), dtype=np.float64)\n",
    "X_train1 = np.hstack((train_offset, X_train))\n",
    "X_test1 = np.hstack((test_offset, X_test))\n",
    "#Linear Regression\n",
    "theta = least_square(X_train1, y_train)\n",
    "LR_predicted_Y_from_trainset = pred_fn(X_train1, theta)\n",
    "LR_rmse_trainset = root_mean_square_error(LR_predicted_Y_from_trainset, y_train)\n",
    "LR_predicted_Y_from_testset = pred_fn(X_test1, theta)\n",
    "LR_rmse_testset = root_mean_square_error(LR_predicted_Y_from_testset, y_test)\n",
    "#Ridge Regression\n",
    "theta_r = ridge_reg(X_train1, y_train, 15)\n",
    "RR_predicted_Y_from_trainset = pred_fn(X_train1, theta_r)\n",
    "RR_rmse_trainset = root_mean_square_error(RR_predicted_Y_from_trainset, y_train)\n",
    "RR_predicted_Y_from_testset = pred_fn(X_test1, theta_r)\n",
    "RR_rmse_testset = root_mean_square_error(RR_predicted_Y_from_testset, y_test)\n",
    "print(LR_rmse_trainset, LR_rmse_testset, RR_rmse_trainset, RR_rmse_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "a= list_train_metric[0]\n",
    "b= list_test_metric[0]\n",
    "\n",
    "labels = ['Least-square', 'Ridge-regression', 'Random-forest']\n",
    "Train_RMSE = [round(LR_rmse_trainset, 2), round(RR_rmse_trainset, 2), round(a, 2)]\n",
    "Test_RMSE = [round(LR_rmse_testset,2), round(RR_rmse_testset, 2), round(b, 2)]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, Train_RMSE, width, label='Train')\n",
    "rects2 = ax.bar(x + width/2, Test_RMSE, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Comparison of Regression Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train_metric = []\n",
    "list_test_metric = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "clf = GBDT(loss='logistic', num_trees=30)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = clf.predict(X_train)\n",
    "acc_train = accuracy(y_train, y_pred_train)\n",
    "print (\"Train Accuracy:\", acc_train)\n",
    "list_train_metric.append(acc_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "acc_test = accuracy(y_test, y_pred_test)\n",
    "print (\"Test Accuracy:\", acc_test)\n",
    "list_test_metric.append(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: GBDT regression on boston house price dataset\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "clf = GBDT(loss='mse', num_trees=30)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = clf.predict(X_train)\n",
    "RMSE_train = root_mean_square_error(y_pred_train, y_train)\n",
    "print (\"Train RMSE:\", RMSE_train)\n",
    "list_train_metric.append(RMSE_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "RMSE_test = root_mean_square_error(y_pred_test, y_test)\n",
    "print (\"Test RMSE:\", RMSE_test)\n",
    "list_test_metric.append(RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "clf = GBDT(loss='logistic', num_trees=30)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = clf.predict(X_train)\n",
    "acc_train = accuracy(y_train, y_pred_train)\n",
    "print (\"Train Accuracy:\", acc_train)\n",
    "list_train_metric.append(acc_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "acc_test = accuracy(y_test, y_pred_test)\n",
    "print (\"Test Accuracy:\", acc_test)\n",
    "list_test_metric.append(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "print('Training metric: ')\n",
    "print(list_train_metric)\n",
    "\n",
    "print('Test metric: ')\n",
    "print(list_test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# TODO: Least Square \n",
    "# load data\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "import numpy as np\n",
    "def least_square(X, y):\n",
    "    #theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "    theta = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)\n",
    "    return theta\n",
    "def ridge_reg(X, y, eta):\n",
    "    n, m = X.shape\n",
    "    I= np.identity(m)\n",
    "    theta_r = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X) + (eta/2) * I), X.T), y)\n",
    "    return theta_r\n",
    "def pred_fn(X, theta):\n",
    "    pred = np.dot(X, theta)\n",
    "    return pred\n",
    "def root_mean_square_error(pred, y):\n",
    "   rmse = np.sqrt(np.square(np.subtract(y, pred)).mean()) \n",
    "   return rmse\n",
    "#Linear Regression\n",
    "train_offset = np.ones((len(X_train), 1), dtype = np.float64)\n",
    "test_offset = np.ones((len(X_test), 1), dtype=np.float64)\n",
    "X_train1 = np.hstack((train_offset, X_train))\n",
    "X_test1 = np.hstack((test_offset, X_test))\n",
    "#Linear Regression\n",
    "theta = least_square(X_train1, y_train)\n",
    "LR_predicted_Y_from_trainset = pred_fn(X_train1, theta)\n",
    "LR_rmse_trainset = root_mean_square_error(LR_predicted_Y_from_trainset, y_train)\n",
    "LR_predicted_Y_from_testset = pred_fn(X_test1, theta)\n",
    "LR_rmse_testset = root_mean_square_error(LR_predicted_Y_from_testset, y_test)\n",
    "#Ridge Regression\n",
    "theta_r = ridge_reg(X_train1, y_train, 15)\n",
    "RR_predicted_Y_from_trainset = pred_fn(X_train1, theta_r)\n",
    "RR_rmse_trainset = root_mean_square_error(RR_predicted_Y_from_trainset, y_train)\n",
    "RR_predicted_Y_from_testset = pred_fn(X_test1, theta_r)\n",
    "RR_rmse_testset = root_mean_square_error(RR_predicted_Y_from_testset, y_test)\n",
    "print(LR_rmse_trainset, LR_rmse_testset, RR_rmse_trainset, RR_rmse_testset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "a= list_train_metric[1]\n",
    "b= list_test_metric[1]\n",
    "\n",
    "labels = ['Least-square', 'Ridge-regression', 'GBDT']\n",
    "Train_RMSE = [round(LR_rmse_trainset, 2), round(RR_rmse_trainset, 2), round(a, 2)]\n",
    "Test_RMSE = [round(LR_rmse_testset,2), round(RR_rmse_testset, 2), round(b, 2)]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, Train_RMSE, width, label='Train')\n",
    "rects2 = ax.bar(x + width/2, Test_RMSE, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Comparison of Regression Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\"\"\"\n",
    "def autolabel(rects):\n",
    "    \n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
